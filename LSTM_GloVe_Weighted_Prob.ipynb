{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":26,"outputs":[{"output_type":"stream","text":"/kaggle/input/glove-global-vectors-for-word-representation/glove.twitter.27B.100d.txt\n/kaggle/input/glove-global-vectors-for-word-representation/glove.twitter.27B.200d.txt\n/kaggle/input/glove-global-vectors-for-word-representation/glove.twitter.27B.25d.txt\n/kaggle/input/glove-global-vectors-for-word-representation/glove.twitter.27B.50d.txt\n/kaggle/input/nlpaug-augmented-data/train_augmented.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n/kaggle/input/nlp-getting-started/sample_submission.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('averaged_perceptron_tagger')\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\n\nfrom wordcloud import WordCloud\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nfrom collections import defaultdict\nfrom collections import Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\n\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\n\nfrom tqdm import tqdm\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout, Bidirectional\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam","execution_count":27,"outputs":[{"output_type":"stream","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('../input')","execution_count":28,"outputs":[{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"['glove-global-vectors-for-word-representation',\n 'nlpaug-augmented-data',\n 'nlp-getting-started']"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### read data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# train_df = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('../input/nlp-getting-started/test.csv')\ntrain_df = pd.read_csv('../input/nlpaug-augmented-data/train_augmented.csv')","execution_count":35,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### create embeddings dictionary by loading Twitter GloVe (27B tokens) with 100-D vectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"import io\n\nembedding_dict={}\nwith open('../input/glove-global-vectors-for-word-representation/glove.twitter.27B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors","execution_count":36,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### tweets augmentation by replacing words with their synonyms using \"nlpaug\" library"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nlpaug.augmenter.word as naw\nimport nlpaug.augmenter.sentence as nas\nimport nlpaug.flow as nafc\nfrom nlpaug.util import Action\n\naug = naw.SynonymAug(aug_src='wordnet')\n\ndef augmented_data (text, aug):\n\taugmented_text2 = aug.augment(text)\n\taugmented_text3 = aug.augment(text)\n\n\treturn augmented_text2,augmented_text3,\n\nfor i in range(train_df.shape[0]):\n\ttext = train_df[\"text\"][i] \n\tlabel = train_df[\"target\"][i]\n\n\tid = int('0'+str(train_df[\"id\"][i]))\n\taugmented1, augmented2 = augmented_data(text, aug)\n\tdf1 = pd.DataFrame({\"id\":[id, id],\"keyword\":[\"\",\"\"],\"location\":[\"\",\"\"],\"text\":[augmented1, augmented2],\"target\":[label, label]})       \n\ttrain_df = train_df.append(df1,ignore_index = True)\n\nprint(train_df.shape[0])\ntrain_df.to_csv(\"train_augmented.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### build vocabulary from text"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_vocab(texts):\n    \"\"\"\n    :param sentences: list of list of words\n    :return: dictionary of words and their count\n    \"\"\"\n    sentences = texts.apply(lambda x: x.split()).values\n    vocab = {}\n    for sentence in tqdm(sentences):\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab","execution_count":38,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### check how much of the vocabulary is covered by the embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"import operator \n\ndef check_coverage(vocab,embeddings_index):\n    a = {}\n    oov = {}\n    k = 0\n    i = 0\n    for word in tqdm(vocab):\n        try:\n            a[word] = embeddings_index[word]\n            k += vocab[word]\n        except:\n\n            oov[word] = vocab[word]\n            i += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n\n    return sorted_x","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = build_vocab(train_df['text'])\noov = check_coverage(vocab,embedding_dict)\noov[:10]","execution_count":40,"outputs":[{"output_type":"stream","text":"100%|██████████| 22839/22839 [00:00<00:00, 127772.65it/s]\n100%|██████████| 50951/50951 [00:00<00:00, 465073.32it/s]","name":"stderr"},{"output_type":"stream","text":"Found embeddings for 25.56% of vocab\nFound embeddings for  71.12% of all text\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"},{"output_type":"execute_result","execution_count":40,"data":{"text/plain":"[('I', 3207),\n ('The', 1743),\n ('\\x89', 1594),\n ('A', 828),\n ('Û_', 696),\n ('27', 638),\n ('objector', 636),\n ('2', 519),\n ('In', 509),\n ('monophosphate', 507)]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### text preprocessing on train and test dataframes"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([train_df,test_df])\n\ndef remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',str(text))\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  \n                           u\"\\U0001F300-\\U0001F5FF\"  \n                           u\"\\U0001F680-\\U0001F6FF\"  \n                           u\"\\U0001F1E0-\\U0001F1FF\"  \n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\ndef lower(text):\n    words = text.split(\" \")\n    lower = \" \".join([w.lower() for w in words])\n    return lower\n\ndf['text']=df['text'].apply(lambda x : remove_URL(x))\ndf['text']=df['text'].apply(lambda x : remove_html(x))\ndf['text']=df['text'].apply(lambda x: remove_emoji(x))\ndf['text']=df['text'].apply(lambda x : remove_punct(x))\ndf['text']=df['text'].apply(lambda x : lower(x))","execution_count":41,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### transform the data to input for LSTM network; truncate sentences more than 32 words long; create embedding matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 32\ntokenizer_obj = Tokenizer()\ntokenizer_obj.fit_on_texts(df['text'].values.tolist()) \nsequences = tokenizer_obj.texts_to_sequences(df['text'].values)\ntext_pad = pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')\nword_index = tokenizer_obj.word_index\n\nnum_words = len(word_index)+1\ndim = 100\n\nunknown_words = np.random.uniform(-1,1,size=dim).astype('float32')\nunknown_words = unknown_words.reshape(1,dim)\n\nembedding_matrix = np.zeros((num_words, dim))\nfor word, i in tqdm(word_index.items()):    \n    if i > num_words:\n        continue\n    emb_vec=embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec\n    else:\n        embedding_matrix[i]=unknown_words\n\nembedding_matrix.shape","execution_count":42,"outputs":[{"output_type":"stream","text":"100%|██████████| 33592/33592 [00:00<00:00, 293164.31it/s]\n","name":"stderr"},{"output_type":"execute_result","execution_count":42,"data":{"text/plain":"(33593, 100)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### define model; a bidirectional LSTM layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Sequential()\n\nembedding = Embedding(num_words,dim,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(Bidirectional(LSTM(256, dropout=0.25, recurrent_dropout=0.2)))\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nprint(model.summary())","execution_count":43,"outputs":[{"output_type":"stream","text":"Model: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_3 (Embedding)      (None, 32, 100)           3359300   \n_________________________________________________________________\nbidirectional_2 (Bidirection (None, 512)               731136    \n_________________________________________________________________\ndense_9 (Dense)              (None, 1024)              525312    \n_________________________________________________________________\ndropout_8 (Dropout)          (None, 1024)              0         \n_________________________________________________________________\ndense_10 (Dense)             (None, 512)               524800    \n_________________________________________________________________\ndropout_9 (Dropout)          (None, 512)               0         \n_________________________________________________________________\ndense_11 (Dense)             (None, 256)               131328    \n_________________________________________________________________\ndropout_10 (Dropout)         (None, 256)               0         \n_________________________________________________________________\ndense_12 (Dense)             (None, 128)               32896     \n_________________________________________________________________\ndropout_11 (Dropout)         (None, 128)               0         \n_________________________________________________________________\ndense_13 (Dense)             (None, 64)                8256      \n_________________________________________________________________\ndropout_12 (Dropout)         (None, 64)                0         \n_________________________________________________________________\ndense_14 (Dense)             (None, 32)                2080      \n_________________________________________________________________\ndropout_13 (Dropout)         (None, 32)                0         \n_________________________________________________________________\ndense_15 (Dense)             (None, 16)                528       \n_________________________________________________________________\ndropout_14 (Dropout)         (None, 16)                0         \n_________________________________________________________________\ndense_16 (Dense)             (None, 1)                 17        \n=================================================================\nTotal params: 5,315,653\nTrainable params: 1,956,353\nNon-trainable params: 3,359,300\n_________________________________________________________________\nNone\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### define callbacks; ModelCheckpoint to save the best model; EarlyStopping to reduce training epochs if no improvement found"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\ndef get_callbacks():\n    path_checkpoint ='checkpoint_keras.h5'  \n    log_dir='logs'   \n    callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n                                         monitor='val_accuracy',\n                                         verbose=1,\n                                         save_weights_only=False,\n                                         save_best_only=True,\n                                         mode='max',\n                                         period=1)\n    callback_early_stopping = EarlyStopping(monitor='val_accuracy',\n                                           patience=5,\n                                           verbose=1)\n    callbacks = [callback_checkpoint, callback_early_stopping]\n    return callbacks","execution_count":44,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### split data into training and validation data with 80-20 split and start training"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = text_pad[:train_df.shape[0]]\ntest = text_pad[train_df.shape[0]:]\n\nX_train,X_test,y_train,y_test=train_test_split(train,train_df['target'].values,test_size=0.2,random_state=40)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)\n\nhistory=model.fit(X_train,y_train,\n                  batch_size=64,\n                  epochs=25,\n                  validation_data=(X_test,y_test),\n                  verbose=1,\n                  callbacks = get_callbacks())","execution_count":45,"outputs":[{"output_type":"stream","text":"Shape of train (18271, 32)\nShape of Validation  (4568, 32)\nTrain on 18271 samples, validate on 4568 samples\nEpoch 1/25\n18271/18271 [==============================] - 57s 3ms/step - loss: 0.5483 - accuracy: 0.7379 - val_loss: 0.4827 - val_accuracy: 0.7721\n\nEpoch 00001: val_accuracy improved from -inf to 0.77211, saving model to checkpoint_keras.h5\nEpoch 2/25\n18271/18271 [==============================] - 56s 3ms/step - loss: 0.4848 - accuracy: 0.7835 - val_loss: 0.4480 - val_accuracy: 0.8045\n\nEpoch 00002: val_accuracy improved from 0.77211 to 0.80451, saving model to checkpoint_keras.h5\nEpoch 3/25\n18271/18271 [==============================] - 56s 3ms/step - loss: 0.4635 - accuracy: 0.7922 - val_loss: 0.4518 - val_accuracy: 0.8032\n\nEpoch 00003: val_accuracy did not improve from 0.80451\nEpoch 4/25\n18271/18271 [==============================] - 56s 3ms/step - loss: 0.4412 - accuracy: 0.8067 - val_loss: 0.4282 - val_accuracy: 0.8146\n\nEpoch 00004: val_accuracy improved from 0.80451 to 0.81458, saving model to checkpoint_keras.h5\nEpoch 5/25\n18271/18271 [==============================] - 56s 3ms/step - loss: 0.4220 - accuracy: 0.8124 - val_loss: 0.4099 - val_accuracy: 0.8198\n\nEpoch 00005: val_accuracy improved from 0.81458 to 0.81983, saving model to checkpoint_keras.h5\nEpoch 6/25\n18271/18271 [==============================] - 56s 3ms/step - loss: 0.3986 - accuracy: 0.8268 - val_loss: 0.3939 - val_accuracy: 0.8275\n\nEpoch 00006: val_accuracy improved from 0.81983 to 0.82750, saving model to checkpoint_keras.h5\nEpoch 7/25\n18271/18271 [==============================] - 56s 3ms/step - loss: 0.3702 - accuracy: 0.8420 - val_loss: 0.3634 - val_accuracy: 0.8398\n\nEpoch 00007: val_accuracy improved from 0.82750 to 0.83975, saving model to checkpoint_keras.h5\nEpoch 8/25\n18271/18271 [==============================] - 56s 3ms/step - loss: 0.3432 - accuracy: 0.8542 - val_loss: 0.3867 - val_accuracy: 0.8356\n\nEpoch 00008: val_accuracy did not improve from 0.83975\nEpoch 9/25\n18271/18271 [==============================] - 56s 3ms/step - loss: 0.3228 - accuracy: 0.8624 - val_loss: 0.3641 - val_accuracy: 0.8277\n\nEpoch 00009: val_accuracy did not improve from 0.83975\nEpoch 10/25\n18271/18271 [==============================] - 56s 3ms/step - loss: 0.2893 - accuracy: 0.8804 - val_loss: 0.3386 - val_accuracy: 0.8590\n\nEpoch 00010: val_accuracy improved from 0.83975 to 0.85902, saving model to checkpoint_keras.h5\nEpoch 11/25\n18271/18271 [==============================] - 58s 3ms/step - loss: 0.2736 - accuracy: 0.8900 - val_loss: 0.3257 - val_accuracy: 0.8636\n\nEpoch 00011: val_accuracy improved from 0.85902 to 0.86362, saving model to checkpoint_keras.h5\nEpoch 12/25\n18271/18271 [==============================] - 56s 3ms/step - loss: 0.2468 - accuracy: 0.9015 - val_loss: 0.3244 - val_accuracy: 0.8625\n\nEpoch 00012: val_accuracy did not improve from 0.86362\nEpoch 13/25\n18271/18271 [==============================] - 56s 3ms/step - loss: 0.2208 - accuracy: 0.9100 - val_loss: 0.3309 - val_accuracy: 0.8778\n\nEpoch 00013: val_accuracy improved from 0.86362 to 0.87785, saving model to checkpoint_keras.h5\nEpoch 14/25\n18271/18271 [==============================] - 61s 3ms/step - loss: 0.2029 - accuracy: 0.9209 - val_loss: 0.3167 - val_accuracy: 0.8667\n\nEpoch 00014: val_accuracy did not improve from 0.87785\nEpoch 15/25\n18271/18271 [==============================] - 56s 3ms/step - loss: 0.1863 - accuracy: 0.9298 - val_loss: 0.2944 - val_accuracy: 0.8829\n\nEpoch 00015: val_accuracy improved from 0.87785 to 0.88288, saving model to checkpoint_keras.h5\nEpoch 16/25\n18271/18271 [==============================] - 57s 3ms/step - loss: 0.1809 - accuracy: 0.9324 - val_loss: 0.2900 - val_accuracy: 0.8859\n\nEpoch 00016: val_accuracy improved from 0.88288 to 0.88595, saving model to checkpoint_keras.h5\nEpoch 17/25\n18271/18271 [==============================] - 56s 3ms/step - loss: 0.1586 - accuracy: 0.9417 - val_loss: 0.3145 - val_accuracy: 0.8840\n\nEpoch 00017: val_accuracy did not improve from 0.88595\nEpoch 18/25\n18271/18271 [==============================] - 56s 3ms/step - loss: 0.1502 - accuracy: 0.9440 - val_loss: 0.3235 - val_accuracy: 0.8908\n\nEpoch 00018: val_accuracy improved from 0.88595 to 0.89076, saving model to checkpoint_keras.h5\nEpoch 19/25\n18271/18271 [==============================] - 56s 3ms/step - loss: 0.1390 - accuracy: 0.9484 - val_loss: 0.3148 - val_accuracy: 0.8914\n\nEpoch 00019: val_accuracy improved from 0.89076 to 0.89142, saving model to checkpoint_keras.h5\nEpoch 20/25\n18271/18271 [==============================] - 57s 3ms/step - loss: 0.1301 - accuracy: 0.9502 - val_loss: 0.3179 - val_accuracy: 0.8919\n\nEpoch 00020: val_accuracy improved from 0.89142 to 0.89186, saving model to checkpoint_keras.h5\nEpoch 21/25\n18271/18271 [==============================] - 57s 3ms/step - loss: 0.1166 - accuracy: 0.9573 - val_loss: 0.3338 - val_accuracy: 0.8984\n\nEpoch 00021: val_accuracy improved from 0.89186 to 0.89842, saving model to checkpoint_keras.h5\nEpoch 22/25\n18271/18271 [==============================] - 56s 3ms/step - loss: 0.1150 - accuracy: 0.9559 - val_loss: 0.3150 - val_accuracy: 0.8995\n\nEpoch 00022: val_accuracy improved from 0.89842 to 0.89952, saving model to checkpoint_keras.h5\nEpoch 23/25\n18271/18271 [==============================] - 56s 3ms/step - loss: 0.1130 - accuracy: 0.9594 - val_loss: 0.3511 - val_accuracy: 0.8886\n\nEpoch 00023: val_accuracy did not improve from 0.89952\nEpoch 24/25\n18271/18271 [==============================] - 56s 3ms/step - loss: 0.1064 - accuracy: 0.9611 - val_loss: 0.3469 - val_accuracy: 0.8925\n\nEpoch 00024: val_accuracy did not improve from 0.89952\nEpoch 25/25\n18271/18271 [==============================] - 61s 3ms/step - loss: 0.1033 - accuracy: 0.9617 - val_loss: 0.3392 - val_accuracy: 0.9039\n\nEpoch 00025: val_accuracy improved from 0.89952 to 0.90390, saving model to checkpoint_keras.h5\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### print evaluation metrics on validation data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\ny_pred = model.predict(X_test)\ny_pred = y_pred.round().astype('int')\n\nprint(metrics.accuracy_score(y_test,y_pred))\nprint(metrics.confusion_matrix(y_test,y_pred))\nprint(metrics.classification_report(y_test,y_pred))","execution_count":46,"outputs":[{"output_type":"stream","text":"0.9038966725043783\n[[2422  192]\n [ 247 1707]]\n              precision    recall  f1-score   support\n\n           0       0.91      0.93      0.92      2614\n           1       0.90      0.87      0.89      1954\n\n    accuracy                           0.90      4568\n   macro avg       0.90      0.90      0.90      4568\nweighted avg       0.90      0.90      0.90      4568\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### predict the labels for tweets in the test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pred_GloVe = model.predict(test)\ntrain_pred_GloVe_int = train_pred_GloVe.round().astype('int')\n\nsubmission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\nsubmission['target'] = train_pred_GloVe_int\nsubmission.to_csv(\"LSTM_Glove_non_augmented.csv\", index=False, header=True)","execution_count":47,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### weighted probability scores implementation; use \"keyword\" column to weigh a real/fake tweet"},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_keywords = train_df.keyword.unique()\nunique_locations = train_df.keyword.unique()\n\ndict_keyword_target = {}\ndict_location_target = {}\n\nfor key in unique_keywords:\n    dict_keyword_target[key] = train_df[train_df['keyword']==key].target.values\nfor key in unique_locations:\n    dict_location_target = train_df[train_df['keyword']==key].target.values\n\ntrain_df['keyword'].fillna('empty',inplace=True)\nans = train_df[train_df['keyword']!='empty'].shape[0]\nprint(ans)\n\ndict_keyword_prob = {}\ndict_location_prob = {}\nfor key in dict_keyword_target.keys():\n    cnt = 0\n    for i in range(len(dict_keyword_target[key])):\n        if(dict_keyword_target[key][i]==1):\n            cnt += 1\n    if(len(dict_keyword_target[key])!=0):\n        dict_keyword_prob[key] = cnt/len(dict_keyword_target[key])\n\nmodel_predictions = model.predict(test)\nfinal_predictions = []\ncount = 0\ntest_df['keyword'].fillna('empty',inplace=True)\ndict_keyword_prob['empty'] = 1\nchange_cnt_to_0 = 0\nchange_cnt_to_1 = 0\n\nfor i in range(len(model_predictions)):\n    count += 1\n    if(((dict_keyword_prob[test_df['keyword'].tolist()[i]])+model_predictions[i])/2 <= 0.5):\n        if(model_predictions[i]>0.5):\n            change_cnt_to_0 += 1\n        final_predictions.append(0)\n    else:\n        if(model_predictions[i]<=0.5):\n            change_cnt_to_1 += 1\n        final_predictions.append(1)\n\nsubmission['target'] = final_predictions\nsubmission.to_csv(\"LSTM_Glove_aug_weighted.csv\", index=False, header=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}