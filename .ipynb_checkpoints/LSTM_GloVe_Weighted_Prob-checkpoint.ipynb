{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "plt.style.use('ggplot')\n",
    "stop=set(stopwords.words('english'))\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "import string\n",
    "\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout, Bidirectional\n",
    "from keras.initializers import Constant\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('../input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv('../input/nlp-getting-started/train.csv')\n",
    "test_df = pd.read_csv('../input/nlp-getting-started/test.csv')\n",
    "train_df = pd.read_csv('../input/nlpaug-augmented-data/train_augmented.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create embeddings dictionary by loading Twitter GloVe (27B tokens) with 100-D vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "embedding_dict={}\n",
    "with open('../input/glove-global-vectors-for-word-representation/glove.twitter.27B.100d.txt','r') as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word=values[0]\n",
    "        vectors=np.asarray(values[1:],'float32')\n",
    "        embedding_dict[word]=vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tweets augmentation by replacing words with their synonyms using \"nlpaug\" library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.flow as nafc\n",
    "from nlpaug.util import Action\n",
    "\n",
    "aug = naw.SynonymAug(aug_src='wordnet')\n",
    "\n",
    "def augmented_data (text, aug):\n",
    "\taugmented_text2 = aug.augment(text)\n",
    "\taugmented_text3 = aug.augment(text)\n",
    "\n",
    "\treturn augmented_text2,augmented_text3,\n",
    "\n",
    "for i in range(train_df.shape[0]):\n",
    "\ttext = train_df[\"text\"][i] \n",
    "\tlabel = train_df[\"target\"][i]\n",
    "\n",
    "\tid = int('0'+str(train_df[\"id\"][i]))\n",
    "\taugmented1, augmented2 = augmented_data(text, aug)\n",
    "\tdf1 = pd.DataFrame({\"id\":[id, id],\"keyword\":[\"\",\"\"],\"location\":[\"\",\"\"],\"text\":[augmented1, augmented2],\"target\":[label, label]})       \n",
    "\ttrain_df = train_df.append(df1,ignore_index = True)\n",
    "\n",
    "print(train_df.shape[0])\n",
    "train_df.to_csv(\"train_augmented.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build vocabulary from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(texts):\n",
    "    \"\"\"\n",
    "    :param sentences: list of list of words\n",
    "    :return: dictionary of words and their count\n",
    "    \"\"\"\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check how much of the vocabulary is covered by the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator \n",
    "\n",
    "def check_coverage(vocab,embeddings_index):\n",
    "    a = {}\n",
    "    oov = {}\n",
    "    k = 0\n",
    "    i = 0\n",
    "    for word in tqdm(vocab):\n",
    "        try:\n",
    "            a[word] = embeddings_index[word]\n",
    "            k += vocab[word]\n",
    "        except:\n",
    "\n",
    "            oov[word] = vocab[word]\n",
    "            i += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(train_df['text'])\n",
    "oov = check_coverage(vocab,embedding_dict)\n",
    "oov[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text preprocessing on train and test dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df,test_df])\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',str(text))\n",
    "\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  \n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  \n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  \n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_punct(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "def lower(text):\n",
    "    words = text.split(\" \")\n",
    "    lower = \" \".join([w.lower() for w in words])\n",
    "    return lower\n",
    "\n",
    "df['text']=df['text'].apply(lambda x : remove_URL(x))\n",
    "df['text']=df['text'].apply(lambda x : remove_html(x))\n",
    "df['text']=df['text'].apply(lambda x: remove_emoji(x))\n",
    "df['text']=df['text'].apply(lambda x : remove_punct(x))\n",
    "df['text']=df['text'].apply(lambda x : lower(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform the data to input for LSTM network; truncate sentences more than 32 words long; create embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 32\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(df['text'].values.tolist()) \n",
    "sequences = tokenizer_obj.texts_to_sequences(df['text'].values)\n",
    "text_pad = pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')\n",
    "word_index = tokenizer_obj.word_index\n",
    "\n",
    "num_words = len(word_index)+1\n",
    "dim = 100\n",
    "\n",
    "unknown_words = np.random.uniform(-1,1,size=dim).astype('float32')\n",
    "unknown_words = unknown_words.reshape(1,dim)\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, dim))\n",
    "for word, i in tqdm(word_index.items()):    \n",
    "    if i > num_words:\n",
    "        continue\n",
    "    emb_vec=embedding_dict.get(word)\n",
    "    if emb_vec is not None:\n",
    "        embedding_matrix[i]=emb_vec\n",
    "    else:\n",
    "        embedding_matrix[i]=unknown_words\n",
    "\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define model; a bidirectional LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "embedding = Embedding(num_words,dim,embeddings_initializer=Constant(embedding_matrix),\n",
    "                   input_length=MAX_LEN,trainable=False)\n",
    "\n",
    "model.add(embedding)\n",
    "model.add(Bidirectional(LSTM(256, dropout=0.25, recurrent_dropout=0.2)))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define callbacks; ModelCheckpoint to save the best model; EarlyStopping to reduce training epochs if no improvement found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "def get_callbacks():\n",
    "    path_checkpoint ='checkpoint_keras.h5'  \n",
    "    log_dir='logs'   \n",
    "    callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n",
    "                                         monitor='val_accuracy',\n",
    "                                         verbose=1,\n",
    "                                         save_weights_only=False,\n",
    "                                         save_best_only=True,\n",
    "                                         mode='max',\n",
    "                                         period=1)\n",
    "    callback_early_stopping = EarlyStopping(monitor='val_accuracy',\n",
    "                                           patience=5,\n",
    "                                           verbose=1)\n",
    "    callbacks = [callback_checkpoint, callback_early_stopping]\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split data into training and validation data with 80-20 split and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = text_pad[:train_df.shape[0]]\n",
    "test = text_pad[train_df.shape[0]:]\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(train,train_df['target'].values,test_size=0.2,random_state=40)\n",
    "print('Shape of train',X_train.shape)\n",
    "print(\"Shape of Validation \",X_test.shape)\n",
    "\n",
    "history=model.fit(X_train,y_train,\n",
    "                  batch_size=64,\n",
    "                  epochs=25,\n",
    "                  validation_data=(X_test,y_test),\n",
    "                  verbose=1,\n",
    "                  callbacks = get_callbacks())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print evaluation metrics on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.round().astype('int')\n",
    "\n",
    "print(metrics.accuracy_score(y_test,y_pred))\n",
    "print(metrics.confusion_matrix(y_test,y_pred))\n",
    "print(metrics.classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict the labels for tweets in the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_GloVe = model.predict(test)\n",
    "train_pred_GloVe_int = train_pred_GloVe.round().astype('int')\n",
    "\n",
    "submission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\n",
    "submission['target'] = train_pred_GloVe_int\n",
    "submission.to_csv(\"LSTM_Glove_non_augmented.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weighted probability scores implementation; use \"keyword\" column to weigh a real/fake tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_keywords = train_df.keyword.unique()\n",
    "unique_locations = train_df.keyword.unique()\n",
    "\n",
    "dict_keyword_target = {}\n",
    "dict_location_target = {}\n",
    "\n",
    "for key in unique_keywords:\n",
    "    dict_keyword_target[key] = train_df[train_df['keyword']==key].target.values\n",
    "for key in unique_locations:\n",
    "    dict_location_target = train_df[train_df['keyword']==key].target.values\n",
    "\n",
    "train_df['keyword'].fillna('empty',inplace=True)\n",
    "ans = train_df[train_df['keyword']!='empty'].shape[0]\n",
    "print(ans)\n",
    "\n",
    "dict_keyword_prob = {}\n",
    "dict_location_prob = {}\n",
    "for key in dict_keyword_target.keys():\n",
    "    cnt = 0\n",
    "    for i in range(len(dict_keyword_target[key])):\n",
    "        if(dict_keyword_target[key][i]==1):\n",
    "            cnt += 1\n",
    "    if(len(dict_keyword_target[key])!=0):\n",
    "        dict_keyword_prob[key] = cnt/len(dict_keyword_target[key])\n",
    "\n",
    "model_predictions = model.predict(test)\n",
    "final_predictions = []\n",
    "count = 0\n",
    "test_df['keyword'].fillna('empty',inplace=True)\n",
    "dict_keyword_prob['empty'] = 1\n",
    "change_cnt_to_0 = 0\n",
    "change_cnt_to_1 = 0\n",
    "\n",
    "for i in range(len(model_predictions)):\n",
    "    count += 1\n",
    "    if(((dict_keyword_prob[test_df['keyword'].tolist()[i]])+model_predictions[i])/2 <= 0.5):\n",
    "        if(model_predictions[i]>0.5):\n",
    "            change_cnt_to_0 += 1\n",
    "        final_predictions.append(0)\n",
    "    else:\n",
    "        if(model_predictions[i]<=0.5):\n",
    "            change_cnt_to_1 += 1\n",
    "        final_predictions.append(1)\n",
    "\n",
    "submission['target'] = final_predictions\n",
    "submission.to_csv(\"LSTM_Glove_aug_weighted.csv\", index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
