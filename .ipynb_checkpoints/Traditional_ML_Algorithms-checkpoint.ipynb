{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wTcwv5FCz1yQ"
   },
   "source": [
    "### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zzaf4F2qz1yU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tanyQAu0z1yd"
   },
   "source": [
    "### read train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "0Y9RnjAqz1yf",
    "outputId": "0673cc53-1169-4c8e-d924-5000247feef8"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "colab_type": "code",
    "id": "UVDOrn200nsL",
    "outputId": "cad85ecc-191a-4674-96a5-0ac640c0d972"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"popular\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bO35gn4-z1yk"
   },
   "source": [
    "### text cleaning and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nQt0pcm_z1ym"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "porter = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# pre-processing of text in 1 single function\n",
    "def preprocess_text(text):\n",
    "    # remove links from text - https\n",
    "    non_http_list = [w for w in text.lower().split() if 'http' not in w]\n",
    "    # remove @ words from tweets\n",
    "    at_removed_list = [w for w in non_http_list if '@' not in w]\n",
    "    # tokenize sentences to words\n",
    "    words = word_tokenize(' '.join(at_removed_list))\n",
    "    # remove punctuation\n",
    "    word_list = [w for w in words if w not in string.punctuation]\n",
    "    # remove stop words\n",
    "    tokens = [w for w in word_list if w not in stop_words]\n",
    "    # stemming\n",
    "    stemmed = [porter.stem(w) for w in tokens]\n",
    "    # lemmatization\n",
    "    lemmatized = \" \".join([lemmatizer.lemmatize(w) for w in tokens])\n",
    "    \n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o3QFcDLjz1yq"
   },
   "outputs": [],
   "source": [
    "# preprocess and cleaned text stored in new column 'transformed_text'\n",
    "train_df['transformed_text'] = train_df['text'].apply(lambda text: preprocess_text(text))\n",
    "test_df['transformed_text'] = test_df['text'].apply(lambda text: preprocess_text(text))\n",
    "\n",
    "# remove NaN values from keyword column\n",
    "train_df['keyword'].fillna('', inplace=True)\n",
    "test_df['keyword'].fillna('', inplace=True)\n",
    "\n",
    "# # add non-null keyword values to tweet text\n",
    "# train_df['transformed_text'] = train_df['transformed_text'] + ' ' + train_df['keyword']\n",
    "# test_df['transformed_text'] = test_df['transformed_text'] + ' ' + test_df['keyword']\n",
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vxd3dTK1czNx"
   },
   "source": [
    "### EDA\n",
    "\n",
    "Reference 1: https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2gryMYKwdCJx"
   },
   "source": [
    "#### pie chart for class distribution in training data\n",
    "\n",
    "We see that the we don't have a class imbalance in the training dataset. The disaster and non-disaster tweets are pretty close to each other in terms of percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "colab_type": "code",
    "id": "8P0B6fF5dR4D",
    "outputId": "ae140d1d-f745-445a-daa5-68ba69549e24"
   },
   "outputs": [],
   "source": [
    "# import round\n",
    "\n",
    "pos_no = round((train_df['target'].value_counts()/train_df['target'].value_counts().sum())*100)\n",
    "\n",
    "train_df['target'].value_counts().plot(kind='pie', labels=['Fake ('+str(int(pos_no[0]))+'%)', 'Real ('+str(int(pos_no[1]))+'%)'])\n",
    "plt.savefig('target_distribution_pie.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ovo4vyY3g1yi"
   },
   "source": [
    "#### keyword and class distribution\n",
    "\n",
    "We see that the there are some keywords where a majority of the tweets are clearly disaster tweets whereas for other keywords, a majority of tweets are non-disaster tweets. However, keywords where a lot of non-disaster tweets are there also relate a lot to disaster reporting and hence, it seems use of keyword for learning the class distribution won't be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "prHc6NUcg5DZ",
    "outputId": "91d7b622-fc07-4815-d650-3b953768180b"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "train_df['target_mean'] = train_df.groupby('keyword')['target'].transform('mean')\n",
    "\n",
    "plt.figure(figsize=(16, 72), dpi=300)\n",
    "\n",
    "sns.countplot(y=train_df.sort_values(by='target_mean', ascending=False)['keyword'], \n",
    "    hue=train_df.sort_values(by='target_mean', ascending=False)['target'])\n",
    "\n",
    "plt.tick_params(axis='x', labelsize=15)\n",
    "plt.tick_params(axis='y', labelsize=12)\n",
    "plt.legend(loc=1)\n",
    "plt.title('Target Distribution in Keywords')\n",
    "plt.savefig('keyword_class.png')\n",
    "# plt.show()\n",
    "\n",
    "train_df.drop(columns=['target_mean'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P5XJIC8Bs6ff"
   },
   "source": [
    "#### bigram analysis\n",
    "\n",
    "It seems that disaster tweets have a certain use of words that would relate to disasters but this is not entirely true. There are a lot of high frequency bigrams which don't really concern disaster. Also, non-disaster tweets have some bigrams such as 'burning building' or 'emergency services' which can relate to the occurrence of a disaster. This means that a rule-based system and just looking at presence of certain words wouldn't really help the model and also be not scalable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "3ciK-YgsuUez",
    "outputId": "f4359da4-b96c-4e7b-bbfb-be0c3bce0361"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "def generate_ngrams(text, n_gram=1):\n",
    "    token = [token for token in text.lower().split(' ') if token != '' if token not in STOPWORDS]\n",
    "    ngrams = zip(*[token[i:] for i in range(n_gram)])\n",
    "    return [' '.join(ngram) for ngram in ngrams]\n",
    "\n",
    "N = 100\n",
    "\n",
    "disaster_bigrams = defaultdict(int)\n",
    "nondisaster_bigrams = defaultdict(int)\n",
    "\n",
    "for tweet in train_df[train_df['target']==1]['transformed_text']:\n",
    "    for word in generate_ngrams(tweet, n_gram=2):\n",
    "        disaster_bigrams[word] += 1\n",
    "        \n",
    "for tweet in train_df[train_df['target']==0]['transformed_text']:\n",
    "    for word in generate_ngrams(tweet, n_gram=2):\n",
    "        nondisaster_bigrams[word] += 1\n",
    "        \n",
    "df_disaster_bigrams = pd.DataFrame(sorted(disaster_bigrams.items(), key=lambda x: x[1])[::-1])\n",
    "df_nondisaster_bigrams = pd.DataFrame(sorted(nondisaster_bigrams.items(), key=lambda x: x[1])[::-1])\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(18, 50), dpi=100)\n",
    "plt.tight_layout()\n",
    "\n",
    "sns.barplot(y=df_disaster_bigrams[0].values[:N], x=df_disaster_bigrams[1].values[:N], ax=axes[0], color='red')\n",
    "sns.barplot(y=df_nondisaster_bigrams[0].values[:N], x=df_nondisaster_bigrams[1].values[:N], ax=axes[1], color='green')\n",
    "\n",
    "for i in range(2):\n",
    "    axes[i].spines['right'].set_visible(False)\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('')\n",
    "    axes[i].tick_params(axis='x', labelsize=13)\n",
    "    axes[i].tick_params(axis='y', labelsize=13)\n",
    "\n",
    "axes[0].set_title(f'Top {N} most common bigrams in Disaster Tweets', fontsize=15)\n",
    "axes[1].set_title(f'Top {N} most common bigrams in Non-disaster Tweets', fontsize=15)\n",
    "\n",
    "plt.savefig('bigram_analysis.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pY3jC-0LfXhM"
   },
   "source": [
    "#### percentage of missing values for locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "AWN03RsXfnVA",
    "outputId": "a5009b43-1c62-4d6c-dc1b-9c9b1c99bda5"
   },
   "outputs": [],
   "source": [
    "train_missing = train_df.location.isnull().sum()\n",
    "train_present = train_df.location.value_counts().sum()\n",
    "\n",
    "test_missing = test_df.location.isnull().sum()\n",
    "test_present = test_df.location.value_counts().sum()\n",
    "\n",
    "print('Missing values in train: %d'%train_missing)\n",
    "print('Present values in train: %d'%train_present)\n",
    "\n",
    "print('Missing values in test: %d'%test_missing)\n",
    "print('Present values in test: %d'%test_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "_NFebZRyom6s",
    "outputId": "1593c864-18d8-45ef-a6db-6fdb2318a873"
   },
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(8,14)\n",
    "\n",
    "x = train_df.location.value_counts().index.tolist()[:20]\n",
    "y = train_df.location.value_counts().tolist()[:20]\n",
    "\n",
    "y_pos = np.arange(len(x))\n",
    "ax.barh(y_pos, y, align='center')\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(x)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('No. of occurrences')\n",
    "ax.set_title('Top 20 Location counts')\n",
    "\n",
    "for i, v in enumerate(y):\n",
    "    ax.text(v + 3, i + .25, str(v), fontweight='bold')\n",
    "\n",
    "plt.savefig('location_discrete.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RfGjUQ1u1tJT"
   },
   "source": [
    "#### finding which keywords are frequent for top 10 locations (USELESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "uzBuMFg313TU",
    "outputId": "450493ca-39c1-4b72-b158-851e1c2201a0"
   },
   "outputs": [],
   "source": [
    "x = train_df.location.value_counts().index.tolist()[:25]\n",
    "y = train_df.location.value_counts().tolist()[:25]\n",
    "\n",
    "missing = train_df.location.isnull().sum()\n",
    "present = train_df.location.value_counts().sum()\n",
    "\n",
    "top_10_keywords = train_df.keyword.value_counts().index.tolist()[:10]\n",
    "top_10_locations = train_df.location.value_counts().index.tolist()[:10]\n",
    "\n",
    "print('keywords:')\n",
    "print(top_10_keywords)\n",
    "print('locations:')\n",
    "print(top_10_locations)\n",
    "\n",
    "all_locations = []\n",
    "true_df = train_df[train_df['target']==1]\n",
    "for loc in top_10_locations:\n",
    "    filter_df = true_df[true_df['location'] == loc]\n",
    "    all_locations.append(filter_df['keyword'].value_counts().index[0])\n",
    "\n",
    "print(all_locations)\n",
    "print('\\n\\n')\n",
    "dict(zip(top_10_locations, all_locations)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8F-IkRkBz1yu"
   },
   "source": [
    "### CountVectorizer -  without using keyword column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jB1_CL28z1yv"
   },
   "outputs": [],
   "source": [
    "# build bag of words model\n",
    "countvec = feature_extraction.text.CountVectorizer()\n",
    "\n",
    "# transform train, test data\n",
    "train_vec = countvec.fit_transform(train_df['transformed_text'])\n",
    "test_vec = countvec.transform(test_df['transformed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "qw3SxBzleOTB",
    "outputId": "89c9c09b-4117-4172-8cdd-c60feb9ccccb"
   },
   "outputs": [],
   "source": [
    "train_vec.toarray()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OozW9oZCGfhD"
   },
   "source": [
    "### TF-IDF vectorizer - without using keyword column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vnmjOvEgGjDd"
   },
   "outputs": [],
   "source": [
    "# build bag of words model\n",
    "tfidfvec = feature_extraction.text.TfidfVectorizer()\n",
    "\n",
    "# transform train, test data\n",
    "train_vec = tfidfvec.fit_transform(train_df['transformed_text'])\n",
    "test_vec = tfidfvec.transform(test_df['transformed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wm6xKf8Ze-Jf",
    "outputId": "1f006b40-a712-4f2a-b951-80af754b3bf2"
   },
   "outputs": [],
   "source": [
    "train_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AuCJbgeFz1zL"
   },
   "source": [
    "### monte-carlo simulation for different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2
    ],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "BEgBlxaWz1zM",
    "outputId": "f73472c4-18c4-47b3-a410-52a87440f1f2"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def fit_predict(model, x_train, y_train, x_test, y_test):\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    return f1_score(y_test, y_pred)\n",
    "\n",
    "models = [LogisticRegression(), LinearSVC(), MultinomialNB(), DecisionTreeClassifier(), KNeighborsClassifier(n_neighbors=5), RandomForestClassifier()]\n",
    "score_list = []\n",
    "for model in models:\n",
    "    score_list.append([])\n",
    "\n",
    "loop = 5\n",
    "for index in range(loop):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(train_vec, train_df['target'], test_size=0.2, shuffle=True, stratify=train_df['target'])\n",
    "    for (i,model) in enumerate(models):\n",
    "        score_list[i].append(fit_predict(model, x_train, y_train, x_test, y_test))\n",
    "\n",
    "scores_final = np.array(score_list).mean(axis=1)\n",
    "d = {'Classifier': ['Logistic Regression', 'Linear SVM', 'Multinomial Naive Bayes', 'Decision Tree', 'KNN', 'Random Forest'], 'Mean F-1 scores': scores_final}\n",
    "models_f1_final = pd.DataFrame(d)\n",
    "models_f1_final.to_csv('all_model_results.csv', index=False)\n",
    "models_f1_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "it41UD5bz1zV"
   },
   "source": [
    "###### The results for the top 4 classifiers are close hence we do hyper-parameter tuning for all of them "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JJCor57fz1zW"
   },
   "source": [
    "### Hyper-parameter tuning for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "p4lOiUhRz1zX",
    "outputId": "94f6b039-eacb-4314-bc91-d561880c645c"
   },
   "outputs": [],
   "source": [
    "# define classifiers\n",
    "logreg = LogisticRegression(max_iter=10000)\n",
    "parameters = {'C':[0.1, 1, 10], 'solver': ['newton-cg', 'lbfgs', 'liblinear']}\n",
    "clf = GridSearchCV(logreg, parameters, cv=10, scoring='f1')\n",
    "\n",
    "# fit classifier to training data\n",
    "clf.fit(train_vec, train_df['target'])\n",
    "\n",
    "print('Best score: %f'%clf.best_score_)\n",
    "print('\\nBest params: '+str(clf.best_params_))\n",
    "\n",
    "# predict labels for test data\n",
    "predictions = clf.predict(test_vec)\n",
    "test_df['target'] = predictions\n",
    "\n",
    "# generate submission csv file\n",
    "# test_df[['id','target']].to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IPOTKJ5T388Z"
   },
   "source": [
    "### Hyper-parameter tuning for Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "jzXWVOOv37_e",
    "outputId": "96b1ceea-b82f-4db2-f1eb-da4285828c3a"
   },
   "outputs": [],
   "source": [
    "# define classifiers\n",
    "# clf = linear_model.RidgeClassifier()\n",
    "# clf = LogisticRegression(solver='lbfgs')\n",
    "parameters = {'C':[0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "svc = LinearSVC(max_iter=10000)\n",
    "clf = GridSearchCV(svc, parameters, cv=10)\n",
    "\n",
    "# # f-1 score for all k-fold runs\n",
    "# scores = model_selection.cross_val_score(clf, x_train, train_df[\"target\"], cv=10, scoring='f1')\n",
    "# print(scores)\n",
    "# print(scores.mean())\n",
    "\n",
    "# fit classifier to training data\n",
    "clf.fit(train_vec, train_df['target'])\n",
    "\n",
    "print('Best score: %f'%clf.best_score_)\n",
    "print('\\nBest params: '+str(clf.best_params_))\n",
    "# print('\\nBest no.of cv splits:%d'%clf.n_splits_)\n",
    "# print('\\nCV results dict:\\n')\n",
    "# print(clf.cv_results_)\n",
    "\n",
    "# # predict labels for test data\n",
    "# predictions = clf.predict(test_vec)\n",
    "# test_df['target'] = predictions\n",
    "\n",
    "# # generate submission csv file\n",
    "# test_df[['id','target']].to_csv('submission.csv',index=False)\n",
    "\n",
    "# # create a dataframe of mean scores for each value of C - Linear SVM\n",
    "# d = {'C': parameters['C'], 'mean test score': clf.cv_results_['mean_test_score'].tolist()}\n",
    "# df = pd.DataFrame(d)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AypyWH_b4EyA"
   },
   "source": [
    "### Hyper-parameter tuning for Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Bt8naXnh4ELw",
    "outputId": "1fc21067-ffed-412d-c52c-0e0dac7c0d4d"
   },
   "outputs": [],
   "source": [
    "parameters = {'alpha':[0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "mnb = MultinomialNB()\n",
    "clf = GridSearchCV(mnb, parameters, cv=10)\n",
    "\n",
    "# fit classifier to training data\n",
    "clf.fit(train_vec, train_df['target'])\n",
    "\n",
    "print('Best score: %f'%clf.best_score_)\n",
    "print('\\nBest params: '+str(clf.best_params_))\n",
    "\n",
    "predictions = clf.predict(test_vec)\n",
    "test_df['target'] = predictions\n",
    "\n",
    "# generate submission csv file\n",
    "test_df[['id','target']].to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iJTfAeoy4xyR"
   },
   "source": [
    "### Hyper-parameter tuning for Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "sYhGRF2340bn",
    "outputId": "b1c87d8a-e30e-4eed-8f40-34588dc64684"
   },
   "outputs": [],
   "source": [
    "parameters = {'criterion': ['gini', 'entropy']}\n",
    "mnb = DecisionTreeClassifier()\n",
    "clf = GridSearchCV(mnb, parameters, cv=10)\n",
    "\n",
    "# fit classifier to training data\n",
    "clf.fit(train_vec, train_df['target'])\n",
    "\n",
    "print('Best score: %f'%clf.best_score_)\n",
    "print('\\nBest params: '+str(clf.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o3OCAvql9_dc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "Vxd3dTK1czNx",
    "2gryMYKwdCJx",
    "P5XJIC8Bs6ff",
    "RfGjUQ1u1tJT"
   ],
   "name": "disaster_prediction.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
